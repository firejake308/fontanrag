# pip install -U "qwen-agent[gui,rag,code_interpreter,mcp]"
# # Or use `pip install -U qwen-agent` for the minimal requirements.
# # The optional requirements, specified in double brackets, are:
# #   [gui] for Gradio-based GUI support;
# #   [rag] for RAG support;
# #   [code_interpreter] for Code Interpreter support;
# #   [mcp] for MCP support.

import pprint
import urllib.parse
import sqlite3
import json5
from qwen_agent.agents import Assistant
from qwen_agent.tools.base import BaseTool, register_tool
from qwen_agent.utils.output_beautify import typewriter_print
from add_generic import add_generic
from search_and_split_generic import search_generic


# Step 1 (Optional): Add a custom tool named `deep_extract`.
@register_tool('deep_extract_condition')
class DeepExtractCondition(BaseTool):
    # The `description` tells the agent the functionality of this tool.
    description = 'Uses an LLM to parse through the most recent note from each patient to search for a given condition, such' \
    ' as "atrial fibrillation" or "cirrhosis". The LLM will consider "Does the note explicitly state that the patient has, or' \
    ' has ever had, {condition}?" when parsing each note. The result will be saved in a database for future use.' \
    ' This tool takes around 4 hours to run, so ask the user if they are able to wait before using it.'
    # The `parameters` tell the agent what input parameters the tool has.
    parameters = [{
        'name': 'condition',
        'type': 'string',
        'description': 'Name of the condition to extract in English, such as "atrial fibrillation" or "blood pressure"',
        'required': True
    }, {
        'name': 'column_name',
        'type': 'string',
        'description': 'Name of the variable to save in the SQLite database. Should be descriptive but not verbose. Alphanumeric characters and underscores only.',
        'required': True
    }]

    def call(self, params: str, **kwargs) -> str:
        # `params` are the arguments generated by the LLM agent.
        condition = json5.loads(params)['condition']
        column_name = json5.loads(params)['column_name']
        add_generic(condition, column_name)
        return f"Added column {column_name} to database"
    
@register_tool("search_notes")
class SearchNotes(BaseTool):
    description = """
    Perform stratified split based on search terms.
    Returns a list containing the text of notes random file contents,
    ensuring files come from different patients. This tool is useful for quickly understanding the data before starting a long-running call like deep_extract_condition
    """

    parameters = [{
        'name': 'search_queries',
        'type': 'list',
        'description': 'List of search queries to look for in the corpus of notes. Searches are case-insensitive and exact-match.',
        'required': True
    }]

    def call(self, params: str, **kwargs) -> str:
        # `params` are the arguments generated by the LLM agent.
        search_queries = json5.loads(params)['search_queries']
        return json5.dumps(search_generic(search_queries))
    
@register_tool("get_database_structure")
class GetDatabaseStructure(BaseTool):
    describe = "Returns information about the tables and columns in the database of structured data that have already been" \
    " extracted from the notes. You should usually check the structure of the database at least once before running any queries against it."
    parameters = []

    def call(self, params: str, **kwargs) -> str:
         # open MedicalRecords.db
        conn = sqlite3.connect("MedicalRecords.db")
        cursor = conn.cursor()
        result = cursor.execute("PRAGMA table_info('notes');")
        # Get column names
        columns = [description[0] for description in result.description]
        
        # Format as string with headers and rows
        output = "PRAGMA table_info('notes');\n\n"
        output += "\t".join(columns) + "\n"
        for row in result.fetchall():
            output += "\t".join(str(val) for val in row) + "\n"
            
        result = cursor.execute("PRAGMA table_info('patients');")
        # Get column names
        columns = [description[0] for description in result.description]
        
        # Format as string with headers and rows
        output += "\n\nPRAGMA table_info('patients');\n\n"
        output += "\t".join(columns) + "\n"
        for row in result.fetchall():
            output += "\t".join(str(val) for val in row) + "\n"

        return output
        

    
@register_tool("query_database")
class QueryDatabase(BaseTool):
    description = "Runs a SQL query against the database of structured fields that have already been extracted from the notes."
    parameters = [{
        'name': 'query',
        'type': 'string',
        'description': 'the SQL query to run against the database',
        'required': True
    }]

    def call(self, params: str, **kwargs) -> str:
        query = json5.loads(params)['query']

        # open MedicalRecords.db
        conn = sqlite3.connect("MedicalRecords.db")
        cursor = conn.cursor()
        result = cursor.execute(query)
        
        # Format results as a string
        rows = result.fetchall()
        if not rows:
            return "No results found"
            
        # Get column names
        columns = [description[0] for description in result.description]
        
        # Format as string with headers and rows
        output = "\t".join(columns) + "\n"
        for row in rows:
            output += "\t".join(str(val) for val in row) + "\n"
            
        return output

# Step 2: Configure the LLM you are using.
llm_cfg = {
    # You need to download Ollama and get it running in the background for this to work:
    'model': 'qwen3:8b',
    'model_server': 'http://localhost:11434/v1/',
    'api_key': 'EMPTY',

    # (Optional) LLM hyperparameters for generation:
    'generate_cfg': {
        'top_p': 0.8
    }
}

# Step 3: Create an agent. Here we use the `Assistant` agent as an example, which is capable of using tools and reading files.
system_instruction = '''You are a research assistant who has access to a large corpus of pediatric cardiology notes for patients who have undergone or will undergo the Fontan procedure. You also have access to a SQLite database with several structured fields that have been extracted from the notes already, and you should check the database before using the deep_extract_condition tool.'''
tools = ['deep_extract_condition', 'search_notes', 'get_database_structure', 'query_database']
bot = Assistant(llm=llm_cfg,
                system_message=system_instruction,
                function_list=tools)

# Step 4: Run the agent as a chatbot.
messages = []  # This stores the chat history.
while True:
    # For example, enter the query "draw a dog and rotate it 90 degrees".
    query = input('\nuser query: ')
    # Append the user query to the chat history.
    messages.append({'role': 'user', 'content': query})
    response = []
    response_plain_text = ''
    print('bot response:')
    for response in bot.run(messages=messages):
        # Streaming output.
        response_plain_text = typewriter_print(response, response_plain_text)
    # Append the bot responses to the chat history.
    messages.extend(response)